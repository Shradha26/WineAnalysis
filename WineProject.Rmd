---
title: "WineProject"
output: html_notebook
---

```{r}
white <- read.csv("wineQualityWhites.csv")
summary(white)

```

```{r}
library(corrplot)
par(mfrow = c(1,1))
cor.temp <- cor(white)
corrplot(cor.temp, method = 'number')
cor(white[,2:12], white$quality)

##Correlations falls between -1 and 1. 0 suggests there is no association between the two variables while numbers close to -1 or 1 suggests a strong negative and positive associations accordingly.

##This also shows that wine quality has positive correlation with alcohol and negative correlation with chlorides and density

```
```{r}
library(ggplot2)
white$rating[5 >= white$quality ] = "Poor"
white$rating[5< white$quality & white$quality < 7] = "Average"
white$rating[7<= white$quality ] = "Good"
white$rating = as.factor(white$rating)

white$rating = relevel(white$rating, "Poor")

ggplot(data = white, aes(x = white$rating)) + 
  geom_bar()
table(white$rating)

##Limiting the quality of wine into three categories of Poor, Good and Great to be able to differntiate patterns in each category.

```

```{r}
library(rpart)
library(rpart.plot)

model1 = rpart(rating ~ . -X -quality, data = white, method="class",cp=0.008)
prp(model1)

##variables to predict quality: alcohol, free sulfur dioxide, pH, sulphate and volatile acidity by adjusting the cp value.


pred1 = predict(model1, type="class")

rpart.plot(model1)
printcp(model1)
plotcp(model1)
summary(model1)

##The complexity parameter is the amount by which splitting that node improved the relative error.
##Confusion Matrix
table(white$rating, pred1)

##Accuracy = (868 + 1651 + 310) / total = 0.58

##Pruning the Tree

##pfit <- prune(model1, cp = model1$cptable[which.min(model1$cptable[,"xerror"]),"CP"])
##prp(pfit)

```


```{r}
##Random Forest Model
##Random forests improve predictive accuracy by generating a large number of bootstrapped trees (based on random samples of variables), classifying a case using each tree in this new "forest", and deciding a final predicted outcome by combining the results across all of the trees (an average in regression, a majority vote in classification). Breiman and Cutler's random forest approach is implimented via the randomForest package.

library(randomForest)

model2 = randomForest(rating ~ . -X -quality, data = white)

pred2 = predict(model2)

print(model2) 
importance(model2)        ##used to find importance of each predictor 

table(white$rating, pred2)

##Accuracy = 0.74
## Significant increase in accuracy with use of this model
```




When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
