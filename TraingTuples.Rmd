---
title: "WineProject"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---

```{r}
white <- read.csv("wineQualityWhites.csv")
summary(white)
train.indeces <- sample(1:nrow(white), 3000)
white.train <- white[train.indeces, ]
white.test <- white[-train.indeces, ]
summary(white.train)
```

```{r}
library(corrplot)
par(mfrow = c(1,1))
cor.temp <- cor(white)
corrplot(cor.temp, method = 'number')
cor(white[,2:12], white$quality)

##Correlations falls between -1 and 1. 0 suggests there is no association between the two variables while numbers close to -1 or 1 suggests a strong negative and positive associations accordingly.

##This also shows that wine quality has positive correlation with alcohol and negative correlation with chlorides and density

```
```{r}
library(ggplot2)
white.train$rating[5 >= white.train$quality ] = "Poor"
white.train$rating[5< white.train$quality & white.train$quality < 7] = "Average"
white.train$rating[7<= white.train$quality ] = "Good"
white.train$rating = as.factor(white.train$rating)

white.train$rating = relevel(white.train$rating, "Poor")

ggplot(data = white.train, aes(x = white.train$rating)) + 
  geom_bar()
table(white.train$rating)

##Limiting the quality of wine into three categories of Poor, Good and Great to be able to differntiate patterns in each category.

white.test$rating[5 >= white.test$quality ] = "Poor"
white.test$rating[5< white.test$quality & white.test$quality < 7] = "Average"
white.test$rating[7<= white.test$quality ] = "Good"
white.test$rating = as.factor(white.test$rating)

white.test$rating = relevel(white.test$rating, "Poor")

```

```{r}
library(rpart)
library(rpart.plot)

model2 = rpart(rating ~ . -X -quality, data = white.train, method="class",cp=0.008)
prp(model2)

##variables to predict quality: alcohol, free sulfur dioxide, pH, sulphate and volatile acidity by adjusting the cp value.

prp(model2)

pred2 = predict(model2, newdata = white.test, type ="class")

rpart.plot(model2)
printcp(model2)
plotcp(model2)
summary(model2)

##The complexity parameter is the amount by which splitting that node improved the relative error.
##Confusion Matrix
table(pred2,white.test$rating)

##Accuracy = (451 +578+151) / 1898 = 0.62

##Pruning the Tree

##pfit <- prune(model1, cp = model1$cptable[which.min(model1$cptable[,"xerror"]),"CP"])
##prp(pfit)

```


```{r}
##Random Forest Model
##Random forests improve predictive accuracy by generating a large number of bootstrapped trees (based on random samples of variables), classifying a case using each tree in this new "forest", and deciding a final predicted outcome by combining the results across all of the trees (an average in regression, a majority vote in classification). Breiman and Cutler's random forest approach is implimented via the randomForest package.

library(randomForest)

model3 = randomForest(rating ~ . -X -quality, data = white.train)

pred3 = predict(model3,newdata=white.test)

print(model3) 
importance(model3)        ##used to find importance of each predictor 

table(white.test$rating, pred3)

##Accuracy = 0.69
## Significant increase in accuracy with use of this model
```

=======

